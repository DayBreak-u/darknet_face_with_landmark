[net]
batch=32
subdivisions=4
# Training
width=416
height=416
#width=608
#height=608
channels=3
momentum=0.949
decay=0.0005
angle=0
saturation = 1.5
exposure = 1.5
hue=.1



learning_rate=0.002
burn_in=1000
;max_batches = 500500
max_batches = 85000
policy=steps
steps=65000,75000
scales=.1,.1

#cutmix=1
#mosaic=1

# conv1
[convolutional]
filters=32
size=3
pad=1
stride=2
batch_normalize=1
activation=relu

# conv2_1_expand
[convolutional]
filters=32
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv2_1_dwise
[convolutional]
groups=32
filters=32
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv2_1_linear
[convolutional]
filters=16
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# conv2_2_expand
[convolutional]
filters=96
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv2_2_dwise
[convolutional]
groups=96
filters=96
size=3
pad=1
stride=2
batch_normalize=1
activation=relu

# conv2_2_linear
[convolutional]
filters=24
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# conv3_1_expand
[convolutional]
filters=144
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv3_1_dwise
[convolutional]
groups=144
filters=144
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv3_1_linear
[convolutional]
filters=24
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_3_1
[shortcut]
from=-4
activation=linear

# conv_3_2_expand
[convolutional]
filters=144
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_3_2_dwise
[convolutional]
groups=144
filters=144
size=3
pad=1
stride=2
batch_normalize=1
activation=relu

# conv_3_2_linear
[convolutional]
filters=32
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# conv_4_1_expand
[convolutional]
filters=192
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_4_1_dwise
[convolutional]
groups=192
filters=192
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_4_1_linear
[convolutional]
filters=32
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_4_1
[shortcut]
from=-4
activation=linear

# conv_4_2_expand
[convolutional]
filters=192
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_4_2_dwise
[convolutional]
groups=192
filters=192
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_4_2_linear
[convolutional]
filters=32
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_4_2
[shortcut]
from=-4
activation=linear

# conv_4_3_expand
[convolutional]
filters=192
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_4_3_dwise
[convolutional]
groups=192
filters=192
size=3
stride=2
pad=1
batch_normalize=1
activation=relu

# conv_4_3_linear
[convolutional]
filters=64
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# conv_4_4_expand
[convolutional]
filters=384
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_4_4_dwise
[convolutional]
groups=384
filters=384
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_4_4_linear
[convolutional]
filters=64
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_4_4
[shortcut]
from=-4
activation=linear

# conv_4_5_expand
[convolutional]
filters=384
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_4_5_dwise
[convolutional]
groups=384
filters=384
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_4_5_linear
[convolutional]
filters=64
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_4_5
[shortcut]
from=-4
activation=linear

# conv_4_6_expand
[convolutional]
filters=384
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_4_6_dwise
[convolutional]
groups=384
filters=384
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_4_6_linear
[convolutional]
filters=64
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_4_6
[shortcut]
from=-4
activation=linear

# conv_4_7_expand
[convolutional]
filters=384
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_4_7_dwise
[convolutional]
groups=384
filters=384
size=3
pad=1
stride=1
batch_normalize=1
activation=relu

# conv_4_7_linear
[convolutional]
filters=96
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# conv_5_1_expand
[convolutional]
filters=576
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_5_1_dwise
[convolutional]
groups=576
filters=576
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_5_1_linear
[convolutional]
filters=96
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_5_1
[shortcut]
from=-4
activation=linear

# conv_5_2_expand
[convolutional]
filters=576
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_5_2_dwise
[convolutional]
groups=576
filters=576
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_5_2_linear
[convolutional]
filters=96
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_5_2
[shortcut]
from=-4
activation=linear

# conv_5_3_expand
[convolutional]
filters=576
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_5_3_dwise
[convolutional]
groups=576
filters=576
size=3
pad=1
stride=2
batch_normalize=1
activation=relu

# conv_5_3_linear
[convolutional]
filters=160
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# conv_6_1_expand
[convolutional]
filters=960
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_6_1_dwise
[convolutional]
groups=960
filters=960
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_6_1_linear
[convolutional]
filters=160
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_6_1
[shortcut]
from=-4
activation=linear

# conv_6_2_expand
[convolutional]
filters=960
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_6_2_dwise
[convolutional]
groups=960
filters=960
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_6_2_linear
[convolutional]
filters=160
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

# block_6_2
[shortcut]
from=-4
activation=linear

# conv_6_3_expand
[convolutional]
filters=960
size=1
stride=1
pad=0
batch_normalize=1
activation=relu

# conv_6_3_dwise
[convolutional]
groups=960
filters=960
size=3
stride=1
pad=1
batch_normalize=1
activation=relu

# conv_6_3_linear
[convolutional]
filters=320
size=1
stride=1
pad=0
batch_normalize=1
activation=linear

##################
#ssh
[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=leaky

[convolutional]
filters=32
batch_normalize=1
size=3
stride=1
pad=1
activation=leaky


[route]
layers = -2

[convolutional]
filters=16
batch_normalize=1
size=3
stride=1
pad=1
activation=leaky

[convolutional]
filters=16
batch_normalize=1
size=3
stride=1
pad=1
activation=leaky

[convolutional]
filters=16
batch_normalize=1
size=3
stride=1
pad=1
activation=leaky

[route]
layers = -3

[convolutional]
filters=16
batch_normalize=1
size=3
stride=1
pad=1
activation=leaky

[route]
layers = -1,-3,-7


[convolutional]
batch_normalize=1
filters=48
size=1
stride=1
pad=1
activation=linear


[yolo]
mask = 6,7,8
anchors = 12,12,  20,20,  32,32,  48,48,  96,96,  128,128,  196,196,  300,300,  512,512
classes=1
num=9
jitter=.35
cls_normalizer=1.0
iou_normalizer=0.02
iou_loss=ciou
ignore_thresh = .5
truth_thresh = 1


[route]
layers = -12

[upsample]
stride=2

[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
activation=leaky

[route]
layers = 48

[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
activation=leaky

[shortcut]
activation=leaky
from=-3

#ssh
[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=leaky

[convolutional]
filters=32
batch_normalize=1
size=3
stride=1
pad=1
activation=leaky


[route]
layers = -2

[convolutional]
filters=16
batch_normalize=1
size=3
stride=1
pad=1
activation=leaky

[convolutional]
filters=16
batch_normalize=1
size=3
stride=1
pad=1
activation=leaky

[convolutional]
filters=16
batch_normalize=1
size=3
stride=1
pad=1
activation=leaky

[route]
layers = -3

[convolutional]
filters=16
batch_normalize=1
size=3
stride=1
pad=1
activation=leaky

[route]
layers = -1,-3,-7


[convolutional]
batch_normalize=1
filters=32
size=1
stride=1
pad=1
activation=linear


[yolo]
mask = 4,5
anchors = 12,12,  20,20,  32,32,  48,48,  96,96,  128,128,  196,196,  300,300,  512,512
classes=1
num=9
jitter=.35
cls_normalizer=1.0
iou_normalizer=0.02
iou_loss=ciou
ignore_thresh = .5
truth_thresh = 1


[route]
layers = -12

[upsample]
stride=2

[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=0
activation=leaky


[route]
layers = 22


[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=0
activation=leaky

[shortcut]
activation=leaky
from=-3


#ssh
[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=leaky

[convolutional]
filters=32
batch_normalize=1
size=3
stride=1
pad=1
activation=leaky


[route]
layers = -2

[convolutional]
filters=16
batch_normalize=1
size=3
stride=1
pad=1
activation=leaky

[convolutional]
filters=16
batch_normalize=1
size=3
stride=1
pad=1
activation=leaky

[convolutional]
filters=16
batch_normalize=1
size=3
stride=1
pad=1
activation=leaky

[route]
layers = -3

[convolutional]
filters=16
batch_normalize=1
size=3
stride=1
pad=1
activation=leaky

[route]
layers = -1,-3,-7


[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=linear


[yolo]
mask = 0,1,2,3
anchors = 12,12,  20,20,  32,32,  48,48,  96,96,  128,128,  196,196,  300,300,  512,512
classes=1
num=9
jitter=.35
cls_normalizer=1.0
iou_normalizer=0.02
iou_loss=ciou
ignore_thresh = .5
truth_thresh = 1
random=0

